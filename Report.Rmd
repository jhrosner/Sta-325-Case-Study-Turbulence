---
title: 'Case Study: Turbulence'
author: "Julia Rosner, Emily Mittleman, Yuzhe Gu, Ashley Chen"
output:
  pdf_document: default
  html_document:
    df_print: paged
date: "2022-11-3"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(broom)
library(stringr)
library(knitr)
library(nnet)
library(ggplot2)
library(MASS)
library(ISLR)
library(leaps)
library(glmnet)
library(mgcv)
library(car)
library(splines)
library(ggpubr)
library(boot)
library(caret)
```

# Introduction

Understanding and predicting turbulence in fluid motion is incredibly important to a vast range of problems, such as air pollution, population dynamics, and weather. However, turbulence looks random, irregular, unpredictable, making it difficult to understand. Thus, our goals are as follows: For a new parameter setting of (Re, Fr, St), predict its particle cluster volume distribution in terms of its four raw moments. Inference: Investigate and interpret how each parameter affects the probability distribution for particle cluster volumes.

# Methodology

#### Inference

Our univariate exploratory data analysis of (Re, Fr, St) and each moment revealed that the R moments are heavily right skewed, which poses a problem to linear regression. We applied log transformations on each moment to obtain more normally distributed variables. We did not change the first raw moment to the central moment, because the mean of the distribution conveys more meaningful information than the central moment, which is always zero. However, we converted the second, third, and fourth raw moments to central moments for inference so that our results are more interpretable. 
  
In addition, despite their numerical natures, Fr and Re only contain three levels ($Fr \in \{0.052, 0.3, \infty\}$ and $Re \in \{90, 224, 398\}$) in the training data. We decided to make them categorical variables for our inference model for 2 main reasons. Firstly, treating them as numeric variables puts our model at risk for extrapolation due to lack of data at many levels of Re and Fr, making our model unable to learn the trends around such regions; and secondly, we believe that these categories could carry real life significance. For instance, Fr = 0.3 is representative of cumulonimbus clouds and 0.052 is representative of cumulus clouds. Focusing on observations collected at such specific levels may lend unique insights into practical problems. 
  
A closer examination of the data revealed interesting interactive patterns among the independent and dependent variables. Specifically, St appears to assume a strong, non-linear relationship with each of the moments. This relationship between St and R moments appeared logarithmic, so we log-transformed St. Also, while treating the Fr variable as categorical, we observed that there is a linear and decreasing relationship between Fr (gravitational acceleration) and R moments. Lastly, there is also a linear and decreasing relationship between Re (Reynolds number) and R moments. We also explored multicollinearity through VIFs for each model, which were very low.
    
Our final models for each of the moments for inference is a cubic polynomical with interaction terms between Re and Fr: 
$$\begin{aligned}
log(R_{moment1})=-2.048+2.002(log(St)) + 0.161(log(St))^2 + 0.446(log(St))^3 \\
-3.824(Re_{224})-6.015(Re_{398})-0.286(Fr_{0.3})-0.317(Fr_{\infty}) \\
+0.238(Re_{224}*Fr_{0.3}) +0.389(Re_{224}*Fr_{\infty})+0.504(Re_{398}*Fr_{\infty})
\end{aligned}$$

$$\begin{aligned}
log(C_{moment2})=6.005+9.517(log(St)) - 4.885(log(St))^2 + 3.907(log(St))^3 \\
-7.520(Re_{224})-11.673(Re_{398})-6.679(Fr_{0.3})-6.562(Fr_{\infty}) \\
+4.612(Re_{224}*Fr_{0.3}) +4.629(Re_{224}*Fr_{\infty})+7.157(Re_{398}*Fr_{\infty})
\end{aligned}$$

$$\begin{aligned}
log(C_{moment3})=14.567+14.025(log(St)) - 8.234(log(St))^2 + 6.315(log(St))^3 \\
-11.292(Re_{224})-17.490(Re_{398})-12.888(Fr_{0.3})-12.625(Fr_{\infty}) \\
+8.726(Re_{224}*Fr_{0.3}) +8.632(Re_{224}*Fr_{\infty})+13.498(Re_{398}*Fr_{\infty})
\end{aligned}$$

$$\begin{aligned}
log(C_{moment4})=23.168+18.0035(log(St)) - 22.202(log(St))^2 + 8.469(log(St))^3 \\
-15.075(Re_{224})-23.325(Re_{398})-19.018(Fr_{0.3})-18.619(Fr_{\infty}) \\
+12.755(Re_{224}*Fr_{0.3}) +12.576(Re_{224}*Fr_{\infty})+19.746(Re_{398}*Fr_{\infty})
\end{aligned}$$

Our goal for inference was to build a model that was both accurate and interpretable. In testing different models, we found that the linear model, while easy to interpret, performed quite poorly. On the other hand, the spline performed quite well, and ever better than the cubic polynomial, but was difficult to interpret. We compared different degrees of polynomial regression models using the validation set approach, and found that for all four moments, the test MSE began to flatten at the third degree. While higher degree polynomial models performed slightly better, we choose degree three avoid the issue of overfitting. Plots of our model predictions on the test set show that the predicted curves fit well to the testing points. We also performed 5-fold cross validation, which showed fairly low RMSE values and high $R^2$ values, further increasing confidence in the model's performance. 

```{r include=FALSE}
data_train <- read.csv("data-train.csv")
data_test <- read.csv("data-test.csv")
```

```{r include=FALSE}
#transforming into centralized moments: keep raw moment 1, change other 3 to centralized 
data_train$C_moment_1 <- data_train$R_moment_1
data_train$C_moment_2 <- data_train$R_moment_2 - (data_train$R_moment_1)^2
data_train$C_moment_3 <- data_train$R_moment_3 - 3 * (data_train$R_moment_1) * (data_train$R_moment_2) + 2 *(data_train$R_moment_1)^2
data_train$C_moment_4 <- data_train$R_moment_4 - 4 * data_train$R_moment_1 * data_train$R_moment_3 + 6 * (data_train$R_moment_1)^2 * data_train$R_moment_2 - 3 * (data_train$R_moment_1)^4
```

```{r include=FALSE}
attach(data_train)
set.seed(3)
train_ind <- sample(x = nrow(data_train), size = 0.8 * nrow(data_train))
test_ind_neg <- -train_ind
training <- data_train[train_ind, ]
testing <- data_train[test_ind_neg, ]
```

```{r include=FALSE, warning=FALSE}
poly.full <- lm(cbind(log(C_moment_1), log(C_moment_2), log(C_moment_3), log(C_moment_4)) ~  poly(log(St), 3) + factor(Re) + factor(Fr) + factor(Re)*factor(Fr), data=data_train)
summary(poly.full)
```

```{r echo=FALSE, warning=FALSE}
# Splines (numeric predictors) (WITH interaction)
poly3m1_inter <- lm(log(C_moment_1) ~ poly(St, 3) + factor(Re) + factor(Fr) + factor(Re)*factor(Fr), data = training)
pred.poly3m1_inter <- predict(poly3m1_inter, testing)

poly3m2_inter <- lm(log(C_moment_2) ~ poly(St, 3) + factor(Re) + factor(Fr) + factor(Re)*factor(Fr), data = training)
pred.poly3m2_inter <- predict(poly3m2_inter, testing)

poly3m3_inter <- lm(log(C_moment_3) ~ poly(St, 3) + factor(Re) + factor(Fr) + factor(Re)*factor(Fr), data = training)
pred.poly3m3_inter <- predict(poly3m3_inter, testing)

poly3m4_inter <- lm(log(C_moment_4) ~ poly(St, 3) + factor(Re) + factor(Fr) + factor(Re)*factor(Fr), data = training)
pred.poly3m4_inter <- predict(poly3m4_inter, testing)

p1 = ggplot(testing, aes(x = St, y = log(C_moment_1), color = factor(Re))) +
  geom_point() +
  geom_line(aes(x = St, y = pred.poly3m1_inter), size = 1) +
  labs(title = "Polynomial Predictions - Moment 1", subtitle = "Fr + Re interaction")
p2 = ggplot(testing, aes(x = St, y = log(C_moment_2), color = factor(Re))) +
  geom_point() +
  geom_line(aes(x = St, y = pred.poly3m2_inter), size = 1) +
  labs(title = "Polynomial Predictions - Moment 2", subtitle = "Fr + Re interaction")
p3 = ggplot(testing, aes(x = St, y = log(C_moment_3), color = factor(Re))) +
  geom_point() +
  geom_line(aes(x = St, y = pred.poly3m3_inter), size = 1) +
  labs(title = "Polynomial Predictions - Moment 3", subtitle = "Fr + Re interaction")
p4 = ggplot(testing, aes(x = St, y = log(C_moment_4), color = factor(Re))) +
  geom_point() +
  geom_line(aes(x = St, y = pred.poly3m4_inter), size = 1) +
  labs(title = "Polynomial Predictions - Moment 4", subtitle = "Fr + Re interaction")
interaction.poly.graphs = ggarrange(p1, p2, p3, p4, ncol=2, nrow=2)
interaction.poly.graphs
```

```{r echo=FALSE, warning=FALSE}
set.seed(123)

train_control_inf <- trainControl(method = "repeatedcv", number = 5)

poly_cv1 <- train(log(C_moment_1) ~ poly(log(St), 3) + factor(Re) + factor(Fr) + factor(Re)*factor(Fr), data = data_train,
               trControl = train_control_inf, method = "lm")
poly_cv2 <- train(log(C_moment_2) ~ poly(log(St), 3) + factor(Re) + factor(Fr) + factor(Re)*factor(Fr), data = data_train,
               trControl = train_control_inf, method = "lm")
poly_cv3 <- train(log(C_moment_3) ~ poly(log(St), 3) + factor(Re) + factor(Fr) + factor(Re)*factor(Fr), data = data_train,
               trControl = train_control_inf, method = "lm")
poly_cv4 <- train(log(C_moment_4) ~ poly(log(St), 3) + factor(Re) + factor(Fr) + factor(Re)*factor(Fr), data = data_train,
               trControl = train_control_inf, method = "lm")

myvars <- c("RMSE", "Rsquared", "MAE")
poly_cv1_results <- poly_cv1$results[myvars]
poly_cv1_results$Moment <- "Moment 1"
poly_cv2_results <- poly_cv2$results[myvars]
poly_cv2_results$Moment <- "Moment 2"
poly_cv3_results <- poly_cv3$results[myvars]
poly_cv3_results$Moment <- "Moment 3"
poly_cv4_results <- poly_cv4$results[myvars]
poly_cv4_results$Moment <- "Moment 4"

cv_results <- rbind(poly_cv1_results, poly_cv2_results, poly_cv3_results, poly_cv4_results)
cv_results_final <- as.data.frame(t(cv_results[,-4]))
colnames(cv_results_final) <- cv_results$Moment
kable(cv_results_final, digits=3)
```

From these models, we can gain insights into how Re, Fr, and St affect the probability distributions for particle cluster volumes. We can see that a larger Reynolds number would lead to lower predictions for all four moments. This means that as the ratio of fluid momentum force to viscous shear force increases, the mean of the distribution would decrease, the variance would decrease, the distribution will skew more to the right, and the tail will be lighter. This is in line with our knowledge that higher Reynolds numbers correspond to greater turbulent behavior. Likewise, an increase in gravitational acceleration decreases all four moments. While interaction effects are usually less interpretable, in this case it makes sense to keep them in the final model. This is because high speeds cause turbulent flow, so the effect of turbulence (Re) on the distribution is different depending on the gravitational acceleration.

#### Prediction

  Despite certain advantages in modeling Re and Fr as categorical variables, we recognize the need to extrapolate beyond the three levels of Re and Fr to model real life circumstances and enable a wider range of prediction. To address the limitations of our current models, we provide the following related models using splines that can be used for extrapolation.
  In these models, Re is numeric, and we transformed Fr to get rid of the infinity value that it takes on.
  
  One of our goals for this case study was to make a model that is specialized in making accurate predictions, meaning that we are not concerned with inference for this particular model. Because we are not concerned with interpreting this model, we explored more complex models because they tend to have better predictive performance than simple models.

In order to predict values for new observations between and/or outside the ranges of values we have seen, we had to make a model that could both interpolate and extrapolate. Instead of using categorical values for our predictor variables Fr and Re, we used numerical values so that our model is more generalizable to accept numerical values between or outside the ranges of Fr and Re that were given in the training data. In order to make Fr a numerical variable, we needed to perform a transformation on it to make Fr take on only finite values for modeling. To do this, we transformed Fr so that the new value is equal to $1 - 2^{-Fr}$, so that the new Fr takes on values between 0 and 1.

DISCUSS PREDICTION SPLINE MODEL W INTERACTIONS

```{r, include=FALSE, results='asis'}
# perform transformation on Fr to get rid of infinity
g <- function(x) {
  return (1 - 2^(-x))
}

printModels <- function(models) {
  for (i in 1:length(models)) {
    print(paste0("R Moment ", i))
    print(knitr::kable(tidy(models[[i]]),
                       digits=3))
    stats = matrix(c(summary(models[[i]])$adj.r.squared, summary(models[[i]])$sigma),
                   ncol=2)
    colnames(stats) = c("Adj R^2", "Std Err")
    print(knitr::kable(stats, digits=3))
    cat('\n\n<!-- -->\n\n')
  }
}
```

```{r, echo=FALSE, message = FALSE, warning = FALSE}
# Splines (numeric predictors) (WITH interaction)
spline1 <- lm(log(R_moment_1) ~ bs(log(St)) + bs(Re)*bs(g(Fr)), data = training)
pred.spline1 <- predict(spline1, testing)
mse_spline1 <- mean((pred.spline1 - log(testing$R_moment_1))^2)
r1=summary(spline1)$adj.r.squared

spline2 <- lm(log(R_moment_2) ~ bs(log(St)) + bs(Re)*bs(g(Fr)), data = training)
pred.spline2 <- predict(spline2, testing)
mse_spline2 <- mean((pred.spline2 - log(testing$R_moment_2))^2)
r2=summary(spline2)$adj.r.squared

spline3 <- lm(log(R_moment_3) ~ bs(log(St)) + bs(Re)*bs(g(Fr)), data = training)
pred.spline3 <- predict(spline3, testing)
mse_spline3 <- mean((pred.spline3 - log(testing$R_moment_3))^2)
r3=summary(spline3)$adj.r.squared

spline4 <- lm(log(R_moment_4) ~ bs(log(St)) + bs(Re)*bs(g(Fr)), data = training)
pred.spline4 <- predict(spline4, testing)
mse_spline4 <- mean((pred.spline4 - log(testing$R_moment_4))^2)
r4=summary(spline4)$adj.r.squared

p1 = ggplot(testing, aes(x = St, y = log(R_moment_1), color = factor(Re))) +
  geom_point() +
  geom_line(aes(x = St, y = pred.spline1), size = 1) +
  labs(title = "Spline Predictions - Moment 1", subtitle = "Fr + Re interaction")
p2 = ggplot(testing, aes(x = St, y = log(R_moment_2), color = factor(Re))) +
  geom_point() +
  geom_line(aes(x = St, y = pred.spline2), size = 1) +
  labs(title = "Spline Predictions - Moment 2", subtitle = "Fr + Re interaction")
p3 = ggplot(testing, aes(x = St, y = log(R_moment_3), color = factor(Re))) +
  geom_point() +
  geom_line(aes(x = St, y = pred.spline3), size = 1) +
  labs(title = "Spline Predictions - Moment 3", subtitle = "Fr + Re interaction")
p4 = ggplot(testing, aes(x = St, y = log(R_moment_4), color = factor(Re))) +
  geom_point() +
  geom_line(aes(x = St, y = pred.spline4), size = 1) +
  labs(title = "Spline Predictions - Moment 4", subtitle = "Fr + Re interaction")
interaction.spline.graphs = ggarrange(p1, p2, p3, p4, ncol=2, nrow=2)
interaction.spline.graphs
```

TALK BRIEFLY ABOUT CROSS VALIDATION
```{r echo=FALSE, warning=FALSE}
# Cross Validation
set.seed(123)

train_control <- trainControl(method = "repeatedcv", number = 5)

spline_cv1 <- train(log(R_moment_1) ~ bs(log(St)) + bs(Re)*bs(g(Fr)), data = data_train,
               trControl = train_control, method = "lm")
spline_cv2 <- train(log(R_moment_2) ~ bs(log(St)) + bs(Re)*bs(g(Fr)), data = data_train,
               trControl = train_control, method = "lm")
spline_cv3 <- train(log(R_moment_3) ~ bs(log(St)) + bs(Re)*bs(g(Fr)), data = data_train,
               trControl = train_control, method = "lm")
spline_cv4 <- train(log(R_moment_4) ~ bs(log(St)) + bs(Re)*bs(g(Fr)), data = data_train,
               trControl = train_control, method = "lm")

myvars <- c("RMSE", "Rsquared", "MAE")
spline_cv1_results <- spline_cv1$results[myvars]
spline_cv1_results$Moment <- "Moment 1"
spline_cv2_results <- spline_cv2$results[myvars]
spline_cv2_results$Moment <- "Moment 2"
spline_cv3_results <- spline_cv4$results[myvars]
spline_cv3_results$Moment <- "Moment 3"
spline_cv4_results <- spline_cv1$results[myvars]
spline_cv4_results$Moment <- "Moment 4"

cv_results <- rbind(spline_cv1_results, spline_cv2_results, spline_cv3_results, spline_cv4_results)
cv_results_new <- as.data.frame(t(cv_results[,-4]))
colnames(cv_results_new) <- cv_results$Moment
kable(cv_results_new, digits=3)
```


## Results
```{r warning=FALSE, echo=FALSE}
#Use previously unseen testing data 
pred.spline1 <- predict(spline1, data_test, se=TRUE)
pred.spline2 <- predict(spline2, data_test, se=TRUE)
pred.spline3 <- predict(spline3, data_test, se=TRUE)
pred.spline4 <- predict(spline4, data_test, se=TRUE)

pred_df <- data.frame(matrix(ncol = 5, nrow = nrow(data_test)))
colnames(pred_df) <- c('R_moment_1', 'R_moment_2', 'R_moment_3', 'R_moment_4')
pred_df$R_moment_1 <- pred.spline1$fit
pred_df$R_moment_2 <- pred.spline2$fit
pred_df$R_moment_3 <- pred.spline3$fit
pred_df$R_moment_4 <- pred.spline4$fit
pred_df$t <- "test"
pred_df <- pred_df %>% discard(~all(is.na(.) | . ==""))

# make CSV to submit with prediction results
pred_result_df <- cbind(data_test, pred_df)
pred_result_df$R_moment_1 <- exp(pred_result_df$R_moment_1)
pred_result_df$R_moment_2 <- exp(pred_result_df$R_moment_2)
pred_result_df$R_moment_3 <- exp(pred_result_df$R_moment_3)
pred_result_df$R_moment_4 <- exp(pred_result_df$R_moment_4)
pred_result_df <- subset(pred_result_df, select=-t)
# write.csv(pred_result_df,"data-test-predictions.csv", row.names = TRUE)

final_pred_df <- cbind(data_test, pred_df)
final_pred_df$Fr <- g(final_pred_df$Fr)

new_data_train <- subset(data_train, select = c(-C_moment_1, -C_moment_2, -C_moment_3, -C_moment_4))
new_data_train$Fr <- g(new_data_train$Fr)
new_data_train$R_moment_1 <- log(R_moment_1)
new_data_train$R_moment_2 <- log(R_moment_2)
new_data_train$R_moment_3 <- log(R_moment_3)
new_data_train$R_moment_4 <- log(R_moment_4)
new_data_train$t <- "train"

final_df <- rbind(new_data_train, final_pred_df)

# pred_df$SE1 <- pred.spline1$se.fit
# pred_df$SE2 <- pred.spline2$se.fit
# pred_df$SE3 <- pred.spline3$se.fit
# pred_df$SE4 <- pred.spline4$se.fit
```
```{r warning=FALSE, include=FALSE}
pred_df$SE1 <- pred.spline1$se.fit
pred_df$SE2 <- pred.spline2$se.fit
pred_df$SE3 <- pred.spline3$se.fit
pred_df$SE4 <- pred.spline4$se.fit
pred_df <- cbind(data_test, pred_df)
```

```{r echo=FALSE, warning=FALSE}
st_p1 <- ggplot(pred_df, aes(x = St, y = R_moment_1)) + 
  geom_point() + 
  geom_errorbar(aes(ymin=R_moment_1-SE1, ymax=R_moment_1+SE1))
st_p2 <- ggplot(pred_df, aes(x = St, y = R_moment_2)) +
  geom_point() +
  geom_errorbar(aes(ymin=R_moment_2-SE2, ymax=R_moment_2+SE2))
st_p3 <- ggplot(pred_df, aes(x = St, y = R_moment_3)) +
  geom_point() +
  geom_errorbar(aes(ymin=R_moment_3-SE3, ymax=R_moment_3+SE3))
st_p4 <- ggplot(pred_df, aes(x = St, y = R_moment_4)) +
  geom_point() +
  geom_errorbar(aes(ymin=R_moment_4-SE4, ymax=R_moment_4+SE4))
ggarrange(st_p1, st_p2, st_p3, st_p4)

# re_p1 <- ggplot(final_df, aes(x = Re, y = R_moment_1, color = t)) + 
#   geom_point()
# re_p2 <- ggplot(final_df, aes(x = Re, y = R_moment_2, color = t)) + 
#   geom_point()
# re_p3 <- ggplot(final_df, aes(x = Re, y = R_moment_3, color = t)) + 
#   geom_point()
# re_p4 <- ggplot(final_df, aes(x = Re, y = R_moment_4, color = t)) + 
#   geom_point()
# ggarrange(re_p1, re_p2, re_p3, re_p4)
# 
# fr_p1 <- ggplot(final_df, aes(x = Fr, y = R_moment_1, color = t)) +
#   geom_point()
# fr_p2 <- ggplot(final_df, aes(x = Fr, y = R_moment_2, color = t)) +
#   geom_point()
# fr_p3 <- ggplot(final_df, aes(x = Fr, y = R_moment_3, color = t)) +
#   geom_point()
# fr_p4 <- ggplot(final_df, aes(x = Fr, y = R_moment_4, color = t)) +
#   geom_point()
# ggarrange(fr_p1, fr_p2, fr_p3, fr_p4)
```
```

## Conclusion
