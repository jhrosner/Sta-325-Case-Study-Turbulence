---
title: 'Case Study: Turbulence'
author: "Julia Rosner, Emily Mittleman, Yuzhe Gu, Ashley Chen"
output:
  pdf_document: default
  html_document:
    df_print: paged
date: "2022-11-3"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(broom)
library(stringr)
library(knitr)
library(nnet)
library(ggplot2)
library(MASS)
library(ISLR)
library(leaps)
library(glmnet)
library(mgcv)
library(car)
library(splines)
library(ggpubr)
library(boot)
library(caret)
```

# Introduction

Understanding and predicting turbulence in fluid motion is incredibly important to a vast range of problems, such as air pollution, population dynamics, and weather. However, turbulence looks random, irregular, unpredictable, making it difficult to understand. Thus, our goals are as follows: For a new parameter setting of (Re, Fr, St), predict its particle cluster volume distribution in terms of its four raw moments. Inference: Investigate and interpret how each parameter affects the probability distribution for particle cluster volumes.

# Methodology

#### Inference

Our univariate exploratory data analysis of (Re, Fr, St) and each moment revealed that the R moments are heavily right skewed, which poses a problem to linear regression. We applied log transformations on each moment to obtain more normally distributed variables. We did not change the first raw moment to the central moment, because the mean of the distribution conveys more meaningful information than the central moment, which is always zero. However, we converted the second, third, and fourth raw moments to central moments for inference so that our results are more interpretable. 
  
In addition, despite their numerical natures, Fr and Re only contain three levels ($Fr \in \{0.052, 0.3, \infty\}$ and $Re \in \{90, 224, 398\}$) in the training data. We decided to make them categorical variables for our inference model for 2 main reasons. Firstly, treating them as numeric variables puts our model at risk for extrapolation due to lack of data at many levels of Re and Fr, making our model unable to learn the trends around such regions; and secondly, we believe that these categories could carry real life significance. For instance, Fr = 0.3 is representative of cumulonimbus clouds and 0.052 is representative of cumulus clouds. Focusing on observations collected at such specific levels may lend unique insights into practical problems. 
  
A closer examination of the data revealed interesting interactive patterns among the independent and dependent variables. Specifically, St appears to assume a strong, non-linear relationship with each of the moments. This relationship between St and R moments appeared logarithmic, so we log-transformed St. Also, while treating the Fr variable as categorical, we observed that there is a linear and decreasing relationship between Fr (gravitational acceleration) and R moments. Lastly, there is also a linear and decreasing relationship between Re (Reynolds number) and R moments. We also explored multicollinearity through VIFs for each model, which were very low.
    
Our final models for each of the moments for inference is a cubic polynomical with interaction terms between Re and Fr: 
$$\begin{aligned}
log(R_{moment1})=-2.048+2.002(log(St)) + 0.161(log(St))^2 + 0.446(log(St))^3 \\
-3.824(Re_{224})-6.015(Re_{398})-0.286(Fr_{0.3})-0.317(Fr_{\infty}) \\
+0.238(Re_{224}*Fr_{0.3}) +0.389(Re_{224}*Fr_{\infty})+0.504(Re_{398}*Fr_{\infty})
\end{aligned}$$

$$\begin{aligned}
log(C_{moment2})=6.005+9.517(log(St)) - 4.885(log(St))^2 + 3.907(log(St))^3 \\
-7.520(Re_{224})-11.673(Re_{398})-6.679(Fr_{0.3})-6.562(Fr_{\infty}) \\
+4.612(Re_{224}*Fr_{0.3}) +4.629(Re_{224}*Fr_{\infty})+7.157(Re_{398}*Fr_{\infty})
\end{aligned}$$

$$\begin{aligned}
log(C_{moment3})=14.567+14.025(log(St)) - 8.234(log(St))^2 + 6.315(log(St))^3 \\
-11.292(Re_{224})-17.490(Re_{398})-12.888(Fr_{0.3})-12.625(Fr_{\infty}) \\
+8.726(Re_{224}*Fr_{0.3}) +8.632(Re_{224}*Fr_{\infty})+13.498(Re_{398}*Fr_{\infty})
\end{aligned}$$

$$\begin{aligned}
log(C_{moment4})=23.168+18.0035(log(St)) - 22.202(log(St))^2 + 8.469(log(St))^3 \\
-15.075(Re_{224})-23.325(Re_{398})-19.018(Fr_{0.3})-18.619(Fr_{\infty}) \\
+12.755(Re_{224}*Fr_{0.3}) +12.576(Re_{224}*Fr_{\infty})+19.746(Re_{398}*Fr_{\infty})
\end{aligned}$$

Our goal for inference was to build a model that was both accurate and interpretable. In testing different models, we found that the linear model, while easy to interpret, performed quite poorly. On the other hand, the spline performed quite well, and ever better than the cubic polynomial, but was difficult to interpret. We compared different degrees of polynomial regression models using the validation set approach, and found that for all four moments, the test MSE began to flatten at the third degree. While higher degree polynomial models performed slightly better, we choose degree three avoid the issue of overfitting. Plots of our model predictions on the test set show that the predicted curves fit well to the testing points. We also performed 5-fold cross validation, which showed fairly low RMSE values and high $R^2$ values, further increasing confidence in the model's performance. 

```{r include=FALSE}
data_train <- read.csv("data-train.csv")
data_test <- read.csv("data-test.csv")
```

```{r include=FALSE}
#transforming into centralized moments: keep raw moment 1, change other 3 to centralized 
data_train$C_moment_1 <- data_train$R_moment_1
data_train$C_moment_2 <- data_train$R_moment_2 - (data_train$R_moment_1)^2
data_train$C_moment_3 <- data_train$R_moment_3 - 3 * (data_train$R_moment_1) * (data_train$R_moment_2) + 2 *(data_train$R_moment_1)^2
data_train$C_moment_4 <- data_train$R_moment_4 - 4 * data_train$R_moment_1 * data_train$R_moment_3 + 6 * (data_train$R_moment_1)^2 * data_train$R_moment_2 - 3 * (data_train$R_moment_1)^4
```

```{r include=FALSE}
attach(data_train)
set.seed(3)
train_ind <- sample(x = nrow(data_train), size = 0.8 * nrow(data_train))
test_ind_neg <- -train_ind
training <- data_train[train_ind, ]
testing <- data_train[test_ind_neg, ]
```

```{r include=FALSE, warning=FALSE}
poly.full <- lm(cbind(log(C_moment_1), log(C_moment_2), log(C_moment_3), log(C_moment_4)) ~  poly(log(St), 3) + factor(Re) + factor(Fr) + factor(Re)*factor(Fr), data=data_train)
summary(poly.full)
```

```{r echo=FALSE, warning=FALSE}
# Splines (numeric predictors) (WITH interaction)
poly3m1_inter <- lm(log(C_moment_1) ~ poly(St, 3) + factor(Re) + factor(Fr) + factor(Re)*factor(Fr), data = training)
pred.poly3m1_inter <- predict(poly3m1_inter, testing)

poly3m2_inter <- lm(log(C_moment_2) ~ poly(St, 3) + factor(Re) + factor(Fr) + factor(Re)*factor(Fr), data = training)
pred.poly3m2_inter <- predict(poly3m2_inter, testing)

poly3m3_inter <- lm(log(C_moment_3) ~ poly(St, 3) + factor(Re) + factor(Fr) + factor(Re)*factor(Fr), data = training)
pred.poly3m3_inter <- predict(poly3m3_inter, testing)

poly3m4_inter <- lm(log(C_moment_4) ~ poly(St, 3) + factor(Re) + factor(Fr) + factor(Re)*factor(Fr), data = training)
pred.poly3m4_inter <- predict(poly3m4_inter, testing)

p1 = ggplot(testing, aes(x = St, y = log(C_moment_1), color = factor(Re))) +
  geom_point() +
  geom_line(aes(x = St, y = pred.poly3m1_inter), size = 1) +
  labs(title = "Polynomial Predictions - Moment 1", subtitle = "Fr + Re interaction")
p2 = ggplot(testing, aes(x = St, y = log(C_moment_2), color = factor(Re))) +
  geom_point() +
  geom_line(aes(x = St, y = pred.poly3m2_inter), size = 1) +
  labs(title = "Polynomial Predictions - Moment 2", subtitle = "Fr + Re interaction")
p3 = ggplot(testing, aes(x = St, y = log(C_moment_3), color = factor(Re))) +
  geom_point() +
  geom_line(aes(x = St, y = pred.poly3m3_inter), size = 1) +
  labs(title = "Polynomial Predictions - Moment 3", subtitle = "Fr + Re interaction")
p4 = ggplot(testing, aes(x = St, y = log(C_moment_4), color = factor(Re))) +
  geom_point() +
  geom_line(aes(x = St, y = pred.poly3m4_inter), size = 1) +
  labs(title = "Polynomial Predictions - Moment 4", subtitle = "Fr + Re interaction")
interaction.poly.graphs = ggarrange(p1, p2, p3, p4, ncol=2, nrow=2)
interaction.poly.graphs
```

```{r echo=FALSE, warning=FALSE}
set.seed(123)

train_control_inf <- trainControl(method = "repeatedcv", number = 5)

poly_cv1 <- train(log(C_moment_1) ~ poly(log(St), 3) + factor(Re) + factor(Fr) + factor(Re)*factor(Fr), data = data_train,
               trControl = train_control_inf, method = "lm")
poly_cv2 <- train(log(C_moment_2) ~ poly(log(St), 3) + factor(Re) + factor(Fr) + factor(Re)*factor(Fr), data = data_train,
               trControl = train_control_inf, method = "lm")
poly_cv3 <- train(log(C_moment_3) ~ poly(log(St), 3) + factor(Re) + factor(Fr) + factor(Re)*factor(Fr), data = data_train,
               trControl = train_control_inf, method = "lm")
poly_cv4 <- train(log(C_moment_4) ~ poly(log(St), 3) + factor(Re) + factor(Fr) + factor(Re)*factor(Fr), data = data_train,
               trControl = train_control_inf, method = "lm")

myvars <- c("RMSE", "Rsquared", "MAE")
poly_cv1_results <- poly_cv1$results[myvars]
poly_cv1_results$Moment <- "Moment 1"
poly_cv2_results <- poly_cv2$results[myvars]
poly_cv2_results$Moment <- "Moment 2"
poly_cv3_results <- poly_cv3$results[myvars]
poly_cv3_results$Moment <- "Moment 3"
poly_cv4_results <- poly_cv4$results[myvars]
poly_cv4_results$Moment <- "Moment 4"

cv_results <- rbind(poly_cv1_results, poly_cv2_results, poly_cv3_results, poly_cv4_results)
cv_results_final <- as.data.frame(t(cv_results[,-4]))
colnames(cv_results_final) <- cv_results$Moment
kable(cv_results_final, digits=3)
```

From these models, we can gain insights into how Re, Fr, and St affect the probability distributions for particle cluster volumes. We can see that a larger Reynolds number would lead to lower predictions for all four moments. This means that as the ratio of fluid momentum force to viscous shear force increases, the mean of the distribution would decrease, the variance would decrease, the distribution will skew more to the right, and the tail will be lighter. This is in line with our knowledge that higher Reynolds numbers correspond to greater turbulent behavior. Likewise, an increase in gravitational acceleration decreases all four moments. While interaction effects are usually less interpretable, in this case it makes sense to keep them in the final model. This is because high speeds cause turbulent flow, so the effect of turbulence (Re) on the distribution is different depending on the gravitational acceleration.

#### Prediction
  
  One of our goals for this case study was to make a model that is specialized in making accurate predictions, meaning that we are not concerned with inference for this particular model. Because we are not concerned with interpreting this model, we explored more complex models because they tend to have better predictive performance than simple models.
  
  Despite certain advantages in modeling Re and Fr as categorical variables, we recognize the need to extrapolate beyond the three levels of Re and Fr to model real life circumstances and enable a wider range of prediction. In order to predict values for new observations between and/or outside the ranges of values we have seen, we had to make a model that could both interpolate and extrapolate. Instead of using categorical values for our predictor variables Fr and Re, we used numerical values so that our model is more generalizable to accept numerical values between or outside the ranges of Fr and Re that were given in the training data. In order to make Fr a numerical variable, we needed to perform a transformation on it to make Fr take on finite values for modeling. To do this, we transformed Fr so that the new value is equal to $1 - 2^{-Fr}$, so that the new Fr takes on values between 0 and 1.
  
  After exploring linear and polynomial models both with and without interaction terms, we were still not satisfied with the accuracy of these models. Since the performance of our simpler models still showed much room for improvement, we further explored cubic splines with an interaction between Fr and Re. We expected the splines to perform better compared to polynomial regression because instead of fitting a single polynomial function to the entire dataset, spline interpolation fits a piecewise continuous function composed of many polynomials to model the data set. 
  
  We did analysis on cubic spline models with and without interaction terms, and as we expected, the model including the interaction between Fr and Re outperformed the model with no interaction. To analyze predictive performance of our spline model, we performed 5-fold cross validation and selected the final model output containing the best parameters selected by cross validation. The cubic spline with the interaction term for Re and Fr outperformed any other model we created thus far, as it had extremely high $R^2$ values and it produced lower test MSEs for all four R moments as compared to any other models. 
  


```{r, include=FALSE}
moments = list(data_train$R_moment_1, data_train$R_moment_2, data_train$R_moment_3, data_train$R_moment_4)

g <- function(x) {
  return (1 - 2^(-x))
}
```

```{r, echo=FALSE, message = FALSE, warning = FALSE}
# Splines (numeric predictors) (WITH interaction)
spline1 <- lm(log(R_moment_1) ~ bs(log(St)) + bs(Re)*bs(g(Fr)), data = training)
prediction <- predict(spline1, testing, se.fit=TRUE)
pred.spline1 <- prediction$fit
se.spline1 <- prediction$se.fit
mse_spline1 <- mean((pred.spline1 - log(testing$R_moment_1))^2)
r1=summary(spline1)$adj.r.squared

spline1_original = spline1

spline2 <- lm(log(R_moment_2) ~ bs(log(St)) + bs(Re)*bs(g(Fr)), data = training)
prediction <- predict(spline2, testing, se.fit=TRUE)
pred.spline2 <- prediction$fit
se.spline2 <- prediction$se.fit
mse_spline2 <- mean((pred.spline2 - log(testing$R_moment_2))^2)
r2=summary(spline2)$adj.r.squared

spline3 <- lm(log(R_moment_3) ~ bs(log(St)) + bs(Re)*bs(g(Fr)), data = training)
prediction <- predict(spline3, testing, se.fit=TRUE)
pred.spline3 <- prediction$fit
se.spline3 <- prediction$se.fit
mse_spline3 <- mean((pred.spline3 - log(testing$R_moment_3))^2)
r3=summary(spline3)$adj.r.squared

spline4 <- lm(log(R_moment_4) ~ bs(log(St)) + bs(Re)*bs(g(Fr)), data = training)
prediction <- predict(spline4, testing, se.fit=TRUE)
pred.spline4 <- prediction$fit
se.spline4 <- prediction$se.fit
mse_spline4 <- mean((pred.spline4 - log(testing$R_moment_4))^2)
r4=summary(spline4)$adj.r.squared

p1 = ggplot(testing, aes(x = St, y = log(R_moment_1), color = factor(Re))) +
  geom_point() +
  geom_line(aes(x = St, y = pred.spline1), size = 1) +
  labs(title = "Spline Predictions - Moment 1", subtitle = "Fr + Re interaction")
p2 = ggplot(testing, aes(x = St, y = log(R_moment_2), color = factor(Re))) +
  geom_point() +
  geom_line(aes(x = St, y = pred.spline2), size = 1) +
  labs(title = "Spline Predictions - Moment 2", subtitle = "Fr + Re interaction")
p3 = ggplot(testing, aes(x = St, y = log(R_moment_3), color = factor(Re))) +
  geom_point() +
  geom_line(aes(x = St, y = pred.spline3), size = 1) +
  labs(title = "Spline Predictions - Moment 3", subtitle = "Fr + Re interaction")
p4 = ggplot(testing, aes(x = St, y = log(R_moment_4), color = factor(Re))) +
  geom_point() +
  geom_line(aes(x = St, y = pred.spline4), size = 1) +
  labs(title = "Spline Predictions - Moment 4", subtitle = "Fr + Re interaction")
interaction.spline.graphs = ggarrange(p1, p2, p3, p4, ncol=2, nrow=2)
interaction.spline.graphs

# Table of Standard Errors for Predictions
se_spline = matrix(c(se.spline1, se.spline2, se.spline3, se.spline4),ncol=4)
colnames(se_spline) = c("Moment 1", "Moment 2", "Moment 3", "Moment 4")
kable(se_spline, digits=3)

# Table of MSE and Adj. R^2
mse_spline = rbind(c(mse_spline1, mse_spline2, mse_spline3, mse_spline4), c(r1, r2, r3, r4))
colnames(mse_spline) = c("Moment 1", "Moment 2", "Moment 3", "Moment 4")
rownames(mse_spline) = c("MSE", "Ajd R^2")
#kable(mse_spline, digits=3)
```

```{r echo=FALSE, warning=FALSE}
# Cross Validation
set.seed(123)

train_control <- trainControl(method = "repeatedcv", number = 5)

spline_cv1 <- train(log(R_moment_1) ~ bs(log(St)) + bs(Re)*bs(g(Fr)), data = data_train,
               trControl = train_control, method = "lm")
spline_cv2 <- train(log(R_moment_2) ~ bs(log(St)) + bs(Re)*bs(g(Fr)), data = data_train,
               trControl = train_control, method = "lm")
spline_cv3 <- train(log(R_moment_3) ~ bs(log(St)) + bs(Re)*bs(g(Fr)), data = data_train,
               trControl = train_control, method = "lm")
spline_cv4 <- train(log(R_moment_4) ~ bs(log(St)) + bs(Re)*bs(g(Fr)), data = data_train,
               trControl = train_control, method = "lm")

r1=summary(spline_cv1$finalModel)$adj.r.squared
r2=summary(spline_cv2$finalModel)$adj.r.squared
r3=summary(spline_cv3$finalModel)$adj.r.squared
r4=summary(spline_cv4$finalModel)$adj.r.squared

mse_spline1=(spline_cv1$results["RMSE"][1,1])^2
mse_spline2=(spline_cv2$results["RMSE"][1,1])^2
mse_spline3=(spline_cv3$results["RMSE"][1,1])^2
mse_spline4=(spline_cv4$results["RMSE"][1,1])^2

mse_spline = rbind(c(mse_spline1, mse_spline2, mse_spline3, mse_spline4), c(r1, r2, r3, r4))
colnames(mse_spline) = c("Moment 1", "Moment 2", "Moment 3", "Moment 4")
rownames(mse_spline) = c("MSE", "Ajd R^2")
kable(mse_spline, digits=3)

myvars <- c("RMSE", "Rsquared", "MAE")
spline_cv1_results <- spline_cv1$results[myvars]
spline_cv1_results$Moment <- "Moment 1"
spline_cv2_results <- spline_cv2$results[myvars]
spline_cv2_results$Moment <- "Moment 2"
spline_cv3_results <- spline_cv4$results[myvars]
spline_cv3_results$Moment <- "Moment 3"
spline_cv4_results <- spline_cv1$results[myvars]
spline_cv4_results$Moment <- "Moment 4"

cv_results <- rbind(spline_cv1_results, spline_cv2_results, spline_cv3_results, spline_cv4_results)
cv_results <- as.data.frame(t(cv_results[,-4]))
colnames(cv_results) <- cv_results$Moment
kable(cv_results, digits=3)
```

##### Prediction 
```{r echo=FALSE, warning=FALSE}
#Use previously unseen testing data
pred.spline1 <- predict(spline_cv1, data_test)
pred.spline2 <- predict(spline_cv2, data_test)
pred.spline3 <- predict(spline_cv3, data_test)
pred.spline4 <- predict(spline_cv4, data_test)

# make CSV to submit with prediction results
pred_df <- data.frame(matrix(ncol = 4, nrow = nrow(data_test)))
colnames(pred_df) <- c('R_moment_1', 'R_moment_2', 'R_moment_3', 'R_moment_4')
pred_df$R_moment_1 <- exp(pred.spline1)
pred_df$R_moment_2 <- exp(pred.spline2)
pred_df$R_moment_3 <- exp(pred.spline3)
pred_df$R_moment_4 <- exp(pred.spline4)
pred_df <- pred_df %>% discard(~all(is.na(.) | . ==""))
pred_result_df <- cbind(data_test, pred_df)
write.csv(pred_result_df,"data-test-predictions.csv", row.names = TRUE)
```

## Results


## Conclusion
