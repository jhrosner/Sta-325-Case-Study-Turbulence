---
title: 'Case Study: Turbulence'
output:
  pdf_document: default
  html_document:
    df_print: paged
date: "2022-10-31"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(broom)
library(stringr)
library(knitr)
library(nnet)
library(ggplot2)
library(MASS)
library(ISLR)
library(leaps)
library(glmnet)
library(mgcv)
library(car)
library(splines)
library(ggpubr)
```

## Introduction


## Load Data
```{r}
data_train <- read.csv("data-train.csv")
data_test <- read.csv("data-test.csv")
```

## Goals (include in writeup)

(1) Prediction: For a new parameter setting of (Re, F r, St), predict its particle cluster volume distribution in terms of its four raw moments.

(2) Inference: Investigate and interpret how each parameter affects the probability distribution for particle cluster volumes.

## Exploratory Data Analysis (dont include in writeup, see methodology)
To begin, we will explore the data to ensure it is fit for modelling, determine inital transformations needed of the data, and determine which model we see would best fit the data.

```{r}
names(data_train)
summary(data_train)
```
### Histograms

```{r}
par(mfrow = c(2, 4))
hist(data_train$R_moment_1, main = paste("R_moment_1 Histogram"))
hist(data_train$R_moment_2, main = paste("R_moment_2 Histogram"))
hist(data_train$R_moment_3, main = paste("R_moment_3 Histogram"))
hist(data_train$R_moment_4, main = paste("R_moment_3 Histogram"))
hist(data_train$Fr, breaks=20, main = paste("Fr Histogram"))
hist(data_train$St, breaks=20, main = paste("St Histogram"))
hist(data_train$Re, breaks=20, main = paste("Re Histogram"))
```

These histograms indicate that each R_moment has distributions that are heavily skewed to the right, because the data has many rows of 0. In R_moment_3 and R_moment_4, we notice that the maximum values are extremely high, while the medians are much smaller in comparison. This could pose a problem to our analysis, thus we believe it is best then to apply a transformation to these variables in order to obtain more accurate analysis.

```{r}
par(mfrow = c(2, 2))
hist(log(data_train$R_moment_1), breaks=20, main = paste("Histogram log(R_moment_1)"))
hist(log(data_train$R_moment_2), breaks=20, main = paste("Histogram log(R_moment_2)"))
hist(log(data_train$R_moment_3), breaks=20, main = paste("Histogram log(R_moment_3)"))
hist(log(data_train$R_moment_4), breaks=20, main = paste("Histogram log(R_moment_4)"))
```
Performing a log transformation on these variables created more normally distributed variables. While not perfectly normal, this is a big improvement to the non-transformed variables. We will use the log version of variables and will reflect these transformed variables as such in our interpretations and analysis. 

Another transformation to consider is to turn Fr and Re into ordered, categorical variables, since they each only have 2 or 3 unique values.

(TODO: Figure out how to handle infinite for Fr)

```{r}
p1 <- ggplot(data=data_train, aes(x = St, y=log(R_moment_1))) + 
  geom_point()

p2 <- ggplot(data=data_train, aes(x = St, y=log(R_moment_2))) + 
  geom_point()

p3 <- ggplot(data=data_train, aes(x = St, y=log(R_moment_3))) + 
  geom_point()

p4 <- ggplot(data=data_train, aes(x = St, y=log(R_moment_4))) + 
  geom_point()

ggarrange(p1, p2, p3, p4)

p5 <- ggplot(data=data_train, aes(x = factor(Re), y=log(R_moment_1))) + 
  geom_point()

p6 <- ggplot(data=data_train, aes(x = factor(Re), y=log(R_moment_2))) + 
  geom_point()

p7 <- ggplot(data=data_train, aes(x = factor(Re), y=log(R_moment_3))) + 
  geom_point()

p8 <- ggplot(data=data_train, aes(x = factor(Re), y=log(R_moment_4))) + 
  geom_point()

ggarrange(p5, p6, p7, p8)

p9 <- ggplot(data=data_train, aes(x = factor(Fr), y=log(R_moment_1))) + 
  geom_point()

p10 <- ggplot(data=data_train, aes(x = factor(Fr), y=log(R_moment_2))) + 
  geom_point()

p11 <- ggplot(data=data_train, aes(x = factor(Fr), y=log(R_moment_3))) + 
  geom_point()

p12 <- ggplot(data=data_train, aes(x = factor(Fr), y=log(R_moment_4))) + 
  geom_point()

ggarrange(p9, p10, p11, p12)
```

```{r}
pairs(data_train)
```
It appears that each R_moment variable has somewhat of a linear relationship with St. Thus, we may want to begin our search for a best model by fitting a linear model.

## Modeling
We will fit a basic linear model onto each log-transformed response variable.

```{r}
model1 <- lm(log(R_moment_1) ~ St + factor(Re) + factor(Fr), data=data_train)
summary(model1)
model2 <- lm(log(R_moment_2) ~ St + factor(Re) + factor(Fr), data=data_train)
summary(model2)
model3 <- lm(log(R_moment_3) ~ St + factor(Re) + factor(Fr), data=data_train)
summary(model3)
model4 <- lm(log(R_moment_4) ~ St + factor(Re) + factor(Fr), data=data_train)
summary(model4)
```


Exploring collinearity:

```{r}
vif(model1)
vif(model2)
vif(model3)
vif(model4)
```
Including all interaction terms:
```{r}
glm.full <- lm(cbind(log(R_moment_1), log(R_moment_2), log(R_moment_3), log(R_moment_4)) ~  (St + factor(Re) + factor(Fr))^2, data=data_train)
summary(glm.full)
```
Re and Fr seem to have significant interaction for all moments, while St and Re only have significant interaction for the first moment. 

A model with the interaction term for Re and Fr:
```{r}
glm.interaction <- lm(cbind(log(R_moment_1), log(R_moment_2), log(R_moment_3), log(R_moment_4)) ~  (St + factor(Re) + factor(Fr) + factor(Re)*factor(Fr)), data=data_train)
# summary(glm.interaction)
glm.interaction

```
Adding the interaction term between Re and Fr improved the fit of the model according to the adjusted R^2 values.

```{r}
# forward.model <- regsubsets(log(R_moment_1) ~ St + factor(Re) + factor(Fr), 
#                             data = data_train)
# 
# summary(forward.model)
# names(forward.model)
# forward.model$rss
```
```{r}

library(boot)
library(caret)
#  + factor(Fr)*St + St*factor(Re)

glm.interaction1 <- lm(log(R_moment_1) ~  St + factor(Re) + factor(Fr) + factor(Re)*factor(Fr), data=data_train)
glm.interaction2 <- lm(log(R_moment_2) ~  St + factor(Re) + factor(Fr) + factor(Re)*factor(Fr), data=data_train)
glm.interaction3 <- lm(log(R_moment_3) ~  St + factor(Re) + factor(Fr) + factor(Re)*factor(Fr), data=data_train)
glm.interaction4 <- lm(log(R_moment_4) ~  St + factor(Re) + factor(Fr) + factor(Re)*factor(Fr), data=data_train)
# 
# set.seed(325)
# 
# # define training control which
# # generates parameters that further
# # control how models are created
# train_control <- trainControl(method = "cv",
#                               number = 10)
# 
# model <- train(log(R_moment_1)~., data = data_train,
#                trControl = train_control,
#                method = "lm")

confint(glm.interaction1)
par(mfrow = c(2, 2))
plot(glm.interaction1)

confint(glm.interaction2)
par(mfrow = c(2, 2))
plot(glm.interaction2)

confint(glm.interaction3)
par(mfrow = c(2, 2))
plot(glm.interaction3)

confint(glm.interaction4)
par(mfrow = c(2, 2))
plot(glm.interaction4)
```


# Split data into training and test sets
```{r}
attach(data_train)
set.seed(3)
train_ind <- sample(x = nrow(data_train), size = 0.8 * nrow(data_train))
test_ind_neg <- -train_ind
training <- data_train[train_ind, ]
testing <- data_train[test_ind_neg, ]
```

# Linear model using least squares & no interaction term 

```{r}
fit.lm1 <- lm(log(R_moment_1) ~ (St + factor(Re) + factor(Fr)), data = training)
pred.lm1 <- predict(fit.lm1, testing)
mse_test1 <- mean((pred.lm1 - log(testing$R_moment_1))^2)
fit.lm2 <- lm(log(R_moment_2) ~ (St + factor(Re) + factor(Fr)), data = training)
pred.lm2 <- predict(fit.lm2, testing)
mse_test2 <- mean((pred.lm2 - log(testing$R_moment_2))^2)
fit.lm3 <- lm(log(R_moment_3) ~ (St + factor(Re) + factor(Fr)), data = training)
pred.lm3 <- predict(fit.lm3, testing)
mse_test3 <- mean((pred.lm3 - log(testing$R_moment_3))^2)
fit.lm4 <- lm(log(R_moment_4) ~ (St + factor(Re) + factor(Fr)), data = training)
pred.lm4 <- predict(fit.lm4, testing)
mse_test4 <- mean((pred.lm4 - log(testing$R_moment_4))^2)
mse_test1
mse_test2
mse_test3
mse_test4
```

# Linear model using least squares & interaction term

```{r}
fit.lm1 <- lm(log(R_moment_1) ~ (St + factor(Re) + factor(Fr) + factor(Re)*factor(Fr)), data = training)
pred.lm1 <- predict(fit.lm1, testing)
mse_test1 <- mean((pred.lm1 - log(testing$R_moment_1))^2)
fit.lm2 <- lm(log(R_moment_2) ~ (St + factor(Re) + factor(Fr) + factor(Re)*factor(Fr)), data = training)
pred.lm2 <- predict(fit.lm2, testing)
mse_test2 <- mean((pred.lm2 - log(testing$R_moment_2))^2)
fit.lm3 <- lm(log(R_moment_3) ~ (St + factor(Re) + factor(Fr) + factor(Re)*factor(Fr)), data = training)
pred.lm3 <- predict(fit.lm3, testing)
mse_test3 <- mean((pred.lm3 - log(testing$R_moment_3))^2)
fit.lm4 <- lm(log(R_moment_4) ~ (St + factor(Re) + factor(Fr) + factor(Re)*factor(Fr)), data = training)
pred.lm4 <- predict(fit.lm4, testing)
mse_test4 <- mean((pred.lm4 - log(testing$R_moment_4))^2)
mse_test1
mse_test2
mse_test3
mse_test4
```
 
Having an interaction term significantly improved the test MSEs of the linear model.

```{r}
#Create 5 equally size folds
set.seed(325)
folds <- cut(seq(1,nrow(data_train)),breaks=5,labels=FALSE)

test_mses_noint <- list()
#Perform 5 fold cross validation
for(i in 1:5){
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- data_train[testIndexes, ]
    trainData <- data_train[-testIndexes, ]
  
    fit.lm1 <- lm(log(R_moment_1) ~ (St + factor(Re) + factor(Fr)), data = trainData)
    pred.lm1 <- predict(fit.lm1, testData)
    mse_test1 <- mean((pred.lm1 - log(testData$R_moment_1))^2)
    fit.lm2 <- lm(log(R_moment_2) ~ (St + factor(Re) + factor(Fr)), data = trainData)
    pred.lm2 <- predict(fit.lm2, testData)
    mse_test2 <- mean((pred.lm2 - log(testData$R_moment_2))^2)
    fit.lm3 <- lm(log(R_moment_3) ~ (St + factor(Re) + factor(Fr)), data = trainData)
    pred.lm3 <- predict(fit.lm3, testData)
    mse_test3 <- mean((pred.lm3 - log(testData$R_moment_3))^2)
    fit.lm4 <- lm(log(R_moment_4) ~ (St + factor(Re) + factor(Fr)), data = trainData)
    pred.lm4 <- predict(fit.lm4, testData)
    mse_test4 <- mean((pred.lm4 - log(testData$R_moment_4))^2)
    mses = list(mse_test1, mse_test2, mse_test3, mse_test4)
    test_mses_noint <- append(test_mses_noint, mses)
}
```

```{r}
df <- as.data.frame(do.call(rbind, test_mses_noint))  
df$moment <- as.factor(c(1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4))
df$fold <- c(1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5)
df
ggplot(df, aes(x = fold, y = V1, color = moment, group = moment)) + 
  geom_point() + 
  geom_line()
```

```{r}
#Create 5 equally size folds
set.seed(325)
folds <- cut(seq(1,nrow(data_train)),breaks=5,labels=FALSE)

test_mses <- list()
#Perform 5 fold cross validation
for(i in 1:5){
    testIndexes <- which(folds==i,arr.ind=TRUE)
    testData <- data_train[testIndexes, ]
    trainData <- data_train[-testIndexes, ]
  
    fit.lm1 <- lm(log(R_moment_1) ~ (St + factor(Re) + factor(Fr) + factor(Re)*factor(Fr)), data = trainData)
    pred.lm1 <- predict(fit.lm1, testData)
    mse_test1 <- mean((pred.lm1 - log(testData$R_moment_1))^2)
    fit.lm2 <- lm(log(R_moment_2) ~ (St + factor(Re) + factor(Fr) + factor(Re)*factor(Fr)), data = trainData)
    pred.lm2 <- predict(fit.lm2, testData)
    mse_test2 <- mean((pred.lm2 - log(testData$R_moment_2))^2)
    fit.lm3 <- lm(log(R_moment_3) ~ (St + factor(Re) + factor(Fr) + factor(Re)*factor(Fr)), data = trainData)
    pred.lm3 <- predict(fit.lm3, testData)
    mse_test3 <- mean((pred.lm3 - log(testData$R_moment_3))^2)
    fit.lm4 <- lm(log(R_moment_4) ~ (St + factor(Re) + factor(Fr) + factor(Re)*factor(Fr)), data = trainData)
    pred.lm4 <- predict(fit.lm4, testData)
    mse_test4 <- mean((pred.lm4 - log(testData$R_moment_4))^2)
    mses = list(mse_test1, mse_test2, mse_test3, mse_test4)
    test_mses <- append(test_mses, mses)
}
```

```{r}
df <- as.data.frame(do.call(rbind, test_mses))  
df$moment <- as.factor(c(1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4))
df$fold <- c(1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5)
df
ggplot(df, aes(x = fold, y = V1, color = moment, group = moment)) + 
  geom_point() + 
  geom_line()
```
```{r}
data_ctrl <- trainControl(method = "cv", number = 5)

model_caret <- train(log(R_moment_1) ~ (St + factor(Re) + factor(Fr) + factor(Re)*factor(Fr)),   # model to fit
                     data = trainData,                        
                     trControl = data_ctrl,              # folds
                     method = "lm",                      # specifying regression model
                     na.action = na.pass)   

model_caret
```


## Polynomial Regression

For each of the four moments, we try to fit a polynomial model based on the degree of the numerical variable, St. We also include the other two factored variables in each model.

First moment:
```{r}
polym1 <- lm(log(R_moment_1) ~ poly(St, 2) + factor(Re) + factor(Fr), data = training)
summary(polym1)
poly2m1 <- lm(log(R_moment_1) ~ poly(St, 3) + factor(Re) + factor(Fr), data = training)
summary(poly2m1)
poly3m1 <- lm(log(R_moment_1) ~ poly(St, 4) + factor(Re) + factor(Fr), data = training)
summary(poly3m1)
poly4m1 <- lm(log(R_moment_1) ~ poly(St, 5) + factor(Re) + factor(Fr), data = training)
summary(poly4m1)
poly5m1 <- lm(log(R_moment_1) ~ poly(St, 6) + factor(Re) + factor(Fr), data = training)
summary(poly5m1)
poly6m1 <- lm(log(R_moment_1) ~ poly(St, 7) + factor(Re) + factor(Fr), data = training)
summary(poly6m1)
poly7m1 <- lm(log(R_moment_1) ~ poly(St, 8) + factor(Re) + factor(Fr), data = training)
summary(poly7m1)
anova(fit.lm1, polym1, poly2m1, poly3m1, poly4m1, poly5m1, poly6m1, poly7m1)
pred.polym1 <- predict(polym1, testing)
pred.poly2m1 <- predict(poly2m1, testing)
pred.poly3m1 <- predict(poly3m1, testing)
pred.poly4m1 <- predict(poly4m1, testing)
pred.poly5m1 <- predict(poly5m1, testing)
pred.poly6m1 <- predict(poly6m1, testing)
pred.poly7m1 <- predict(poly7m1, testing)
mse_polym1 <- mean((pred.polym1 - log(testing$R_moment_1))^2)
mse_poly2m1 <- mean((pred.poly2m1 - log(testing$R_moment_1))^2)
mse_poly3m1 <- mean((pred.poly3m1 - log(testing$R_moment_1))^2)
mse_poly4m1 <- mean((pred.poly4m1 - log(testing$R_moment_1))^2)
mse_poly5m1 <- mean((pred.poly5m1 - log(testing$R_moment_1))^2)
mse_poly6m1 <- mean((pred.poly6m1 - log(testing$R_moment_1))^2)
mse_poly7m1 <- mean((pred.poly7m1 - log(testing$R_moment_1))^2)
mse_polym1
mse_poly2m1
mse_poly3m1
mse_poly4m1
mse_poly5m1
mse_poly6m1
mse_poly7m1
```
Similar to least squares.

Second moment:
```{r}
polym2 <- lm(log(R_moment_2) ~ poly(St, 2) + factor(Re) + factor(Fr), data = training)
summary(polym2)
poly2m2 <- lm(log(R_moment_2) ~ poly(St, 3) + factor(Re) + factor(Fr), data = training)
summary(poly2m2)
poly3m2 <- lm(log(R_moment_2) ~ poly(St, 4) + factor(Re) + factor(Fr), data = training)
summary(poly3m2)
poly4m2 <- lm(log(R_moment_2) ~ poly(St, 5) + factor(Re) + factor(Fr), data = training)
summary(poly4m2)
poly5m2 <- lm(log(R_moment_2) ~ poly(St, 6) + factor(Re) + factor(Fr), data = training)
summary(poly5m2)
poly6m2 <- lm(log(R_moment_2) ~ poly(St, 7) + factor(Re) + factor(Fr), data = training)
summary(poly6m2)
poly7m2 <- lm(log(R_moment_2) ~ poly(St, 8) + factor(Re) + factor(Fr), data = training)
summary(poly7m2)
anova(fit.lm2, polym2, poly2m2, poly3m2, poly4m2, poly5m2, poly6m2, poly7m2)
pred.polym2 <- predict(polym2, testing)
pred.poly2m2 <- predict(poly2m2, testing)
pred.poly3m2 <- predict(poly3m2, testing)
pred.poly4m2 <- predict(poly4m2, testing)
pred.poly5m2 <- predict(poly5m2, testing)
pred.poly6m2 <- predict(poly6m2, testing)
pred.poly7m2 <- predict(poly7m2, testing)
mse_polym2 <- mean((pred.polym2 - log(testing$R_moment_2))^2)
mse_poly2m2 <- mean((pred.poly2m2 - log(testing$R_moment_2))^2)
mse_poly3m2 <- mean((pred.poly3m2 - log(testing$R_moment_2))^2)
mse_poly4m2 <- mean((pred.poly4m2 - log(testing$R_moment_2))^2)
mse_poly5m2 <- mean((pred.poly5m2 - log(testing$R_moment_2))^2)
mse_poly6m2 <- mean((pred.poly6m2 - log(testing$R_moment_2))^2)
mse_poly7m2 <- mean((pred.poly7m2 - log(testing$R_moment_2))^2)
mse_test2
mse_polym2
mse_poly2m2
mse_poly3m2
mse_poly4m2
mse_poly5m2
mse_poly6m2
mse_poly7m2
```
Same as linear regression? Polynomial model with degree 7 has lowest MSE, but degree 5 or LSR may be better based on ANOVA.

Third moment:
```{r}
polym3 <- lm(log(R_moment_3) ~ poly(St, 2) + factor(Re) + factor(Fr), data = training)
summary(polym3)
poly2m3 <- lm(log(R_moment_3) ~ poly(St, 3) + factor(Re) + factor(Fr), data = training)
summary(poly2m3)
poly3m3 <- lm(log(R_moment_3) ~ poly(St, 4) + factor(Re) + factor(Fr), data = training)
summary(poly3m3)
poly4m3 <- lm(log(R_moment_3) ~ poly(St, 5) + factor(Re) + factor(Fr), data = training)
summary(poly4m3)
poly5m3 <- lm(log(R_moment_3) ~ poly(St, 6) + factor(Re) + factor(Fr), data = training)
summary(poly5m3)
poly6m3 <- lm(log(R_moment_3) ~ poly(St, 7) + factor(Re) + factor(Fr), data = training)
summary(poly6m3)
poly7m3 <- lm(log(R_moment_3) ~ poly(St, 8) + factor(Re) + factor(Fr), data = training)
summary(poly7m3)
poly8m3 <- lm(log(R_moment_3) ~ poly(St, 9) + factor(Re) + factor(Fr), data = training)
summary(poly8m3)
anova(fit.lm3, polym3, poly2m3, poly3m3, poly4m3, poly5m3, poly6m3, poly7m3, poly8m3)
pred.polym3 <- predict(polym3, testing)
pred.poly2m3 <- predict(poly2m3, testing)
pred.poly3m3 <- predict(poly3m3, testing)
pred.poly4m3 <- predict(poly4m3, testing)
pred.poly5m3 <- predict(poly5m3, testing)
pred.poly6m3 <- predict(poly6m3, testing)
pred.poly7m3 <- predict(poly7m3, testing)
pred.poly8m3 <- predict(poly8m3, testing)
mse_polym3 <- mean((pred.polym3 - log(testing$R_moment_3))^2)
mse_poly2m3 <- mean((pred.poly2m3 - log(testing$R_moment_3))^2)
mse_poly3m3 <- mean((pred.poly3m3 - log(testing$R_moment_3))^2)
mse_poly4m3 <- mean((pred.poly4m3 - log(testing$R_moment_3))^2)
mse_poly5m3 <- mean((pred.poly5m3 - log(testing$R_moment_3))^2)
mse_poly6m3 <- mean((pred.poly6m3 - log(testing$R_moment_3))^2)
mse_poly7m3 <- mean((pred.poly7m3 - log(testing$R_moment_3))^2)
mse_poly8m3 <- mean((pred.poly8m3 - log(testing$R_moment_3))^2)
mse_test3
mse_polym3
mse_poly2m3
mse_poly3m3
mse_poly4m3
mse_poly5m3
mse_poly6m3
mse_poly7m3
mse_poly8m3
```

Seem to be slightly worse than linear regression. Optimal model in terms of MSE still seems to be Least Squares.


Fourth moment:
```{r}
polym4 <- lm(log(R_moment_4) ~ poly(St, 2) + factor(Re) + factor(Fr), data = training)
summary(polym4)
poly2m4 <- lm(log(R_moment_4) ~ poly(St, 3) + factor(Re) + factor(Fr), data = training)
summary(poly2m4)
poly3m4 <- lm(log(R_moment_4) ~ poly(St, 4) + factor(Re) + factor(Fr), data = training)
summary(poly3m4)
poly4m4 <- lm(log(R_moment_4) ~ poly(St, 5) + factor(Re) + factor(Fr), data = training)
summary(poly4m4)
poly5m4 <- lm(log(R_moment_4) ~ poly(St, 6) + factor(Re) + factor(Fr), data = training)
summary(poly5m4)
poly6m4 <- lm(log(R_moment_4) ~ poly(St, 7) + factor(Re) + factor(Fr), data = training)
summary(poly6m4)
poly7m4 <- lm(log(R_moment_4) ~ poly(St, 8) + factor(Re) + factor(Fr), data = training)
summary(poly7m4)
poly8m4 <- lm(log(R_moment_4) ~ poly(St, 8) + factor(Re) + factor(Fr), data = training)
summary(poly8m4)
anova(fit.lm4, polym4, poly2m4, poly3m4, poly4m4, poly5m4, poly6m4, poly7m4, poly8m4)
pred.polym4 <- predict(polym4, testing)
pred.poly2m4 <- predict(poly2m4, testing)
pred.poly3m4 <- predict(poly3m4, testing)
pred.poly4m4 <- predict(poly4m4, testing)
pred.poly5m4 <- predict(poly5m4, testing)
pred.poly6m4 <- predict(poly6m4, testing)
pred.poly7m4 <- predict(poly7m4, testing)
pred.poly8m4 <- predict(poly8m4, testing)
mse_polym4 <- mean((pred.polym4 - log(testing$R_moment_4))^2)
mse_poly2m4 <- mean((pred.poly2m4 - log(testing$R_moment_4))^2)
mse_poly3m4 <- mean((pred.poly3m4 - log(testing$R_moment_4))^2)
mse_poly4m4 <- mean((pred.poly4m4 - log(testing$R_moment_4))^2)
mse_poly5m4 <- mean((pred.poly5m4 - log(testing$R_moment_4))^2)
mse_poly6m4 <- mean((pred.poly6m4 - log(testing$R_moment_4))^2)
mse_poly7m4 <- mean((pred.poly7m4 - log(testing$R_moment_4))^2)
mse_poly8m4 <- mean((pred.poly8m4 - log(testing$R_moment_4))^2)
mse_test4
mse_polym4
mse_poly2m4
mse_poly3m4
mse_poly4m4
mse_poly5m4
mse_poly6m4
mse_poly7m4
mse_poly8m4
```
The linear regression fit seems to have the minimal MSE for the fourth order.


### Splines

First moment:
```{r}
spline1 <- lm(log(R_moment_1) ~ bs(log(St)) + factor(Re) + factor(Fr), data = training)
summary(spline1)
pred.spline1 <- predict(spline1, testing)
mse_spline1 <- mean((pred.spline1 - log(testing$R_moment_1))^2)
spline2 <- lm(log(R_moment_1) ~ bs(log(St), df=4) + factor(Re) + factor(Fr), data = training)
summary(spline2)
pred.spline2 <- predict(spline2, testing)
mse_spline2 <- mean((pred.spline2 - log(testing$R_moment_1))^2)
spline3 <- lm(log(R_moment_1) ~ bs(log(St), df=5) + factor(Re) + factor(Fr), data = training)
summary(spline3)
pred.spline3 <- predict(spline3, testing)
mse_spline3 <- mean((pred.spline3 - log(testing$R_moment_1))^2)
spline4 <- lm(log(R_moment_1) ~ bs(log(St), df=6) + factor(Re) + factor(Fr), data = training)
summary(spline4)
pred.spline4 <- predict(spline4, testing)
mse_spline4 <- mean((pred.spline4 - log(testing$R_moment_1))^2)
spline5 <- lm(log(R_moment_1) ~ bs(log(St), df=7) + factor(Re) + factor(Fr), data = training)
summary(spline5)
pred.spline5 <- predict(spline5, testing)
mse_spline5 <- mean((pred.spline5 - log(testing$R_moment_1))^2)
mse_spline1
mse_spline2
mse_spline3
mse_spline4
mse_spline5
```

Second moment:
```{r}
spline1m2 <- lm(log(R_moment_2) ~ bs(log(St)) + factor(Re) + factor(Fr), data = training)
summary(spline1m2)
pred.spline1m2 <- predict(spline1m2, testing)
mse_spline1m2 <- mean((pred.spline1m2 - log(testing$R_moment_2))^2)
spline2m2 <- lm(log(R_moment_2) ~ bs(log(St), df=4) + factor(Re) + factor(Fr), data = training)
summary(spline2m2)
pred.spline2m2 <- predict(spline2m2, testing)
mse_spline2m2 <- mean((pred.spline2m2 - log(testing$R_moment_2))^2)
spline3m2 <- lm(log(R_moment_2) ~ bs(log(St), df=5) + factor(Re) + factor(Fr), data = training)
summary(spline3m2)
pred.spline3m2 <- predict(spline3m2, testing)
mse_spline3m2 <- mean((pred.spline3m2 - log(testing$R_moment_2))^2)
spline4m2 <- lm(log(R_moment_2) ~ bs(log(St), df=6) + factor(Re) + factor(Fr), data = training)
summary(spline4m2)
pred.spline4m2 <- predict(spline4m2, testing)
mse_spline4m2 <- mean((pred.spline4m2 - log(testing$R_moment_2))^2)
spline5m2 <- lm(log(R_moment_2) ~ bs(log(St), df=7) + factor(Re) + factor(Fr), data = training)
summary(spline5m2)
pred.spline5m2 <- predict(spline5m2, testing)
mse_spline5m2 <- mean((pred.spline5m2 - log(testing$R_moment_2))^2)
mse_spline1m2
mse_spline2m2
mse_spline3m2
mse_spline4m2
mse_spline5m2
```

Third moment:
```{r}
spline1m3 <- lm(log(R_moment_3) ~ bs(log(St)) + factor(Re) + factor(Fr), data = training)
summary(spline1m3)
pred.spline1m3 <- predict(spline1m3, testing)
mse_spline1m3 <- mean((pred.spline1m3 - log(testing$R_moment_3))^2)
spline2m3 <- lm(log(R_moment_3) ~ bs(log(St), df=4) + factor(Re) + factor(Fr), data = training)
summary(spline2m3)
pred.spline2m3 <- predict(spline2m3, testing)
mse_spline2m3 <- mean((pred.spline2m3 - log(testing$R_moment_3))^2)
spline3m3 <- lm(log(R_moment_3) ~ bs(log(St), df=5) + factor(Re) + factor(Fr), data = training)
summary(spline3m3)
pred.spline3m3 <- predict(spline3m3, testing)
mse_spline3m3 <- mean((pred.spline3m3 - log(testing$R_moment_3))^2)
spline4m3 <- lm(log(R_moment_3) ~ bs(log(St), df=6) + factor(Re) + factor(Fr), data = training)
summary(spline4m3)
pred.spline4m3 <- predict(spline4m3, testing)
mse_spline4m3 <- mean((pred.spline4m3 - log(testing$R_moment_3))^2)
spline5m3 <- lm(log(R_moment_3) ~ bs(log(St), df=7) + factor(Re) + factor(Fr), data = training)
summary(spline5m3)
pred.spline5m3 <- predict(spline5m3, testing)
mse_spline5m3 <- mean((pred.spline5m3 - log(testing$R_moment_3))^2)
mse_spline1m3
mse_spline2m3
mse_spline3m3
mse_spline4m3
mse_spline5m3
```


Fourth moment:
```{r}
spline1m4 <- lm(log(R_moment_4) ~ bs(log(St)) + factor(Re) + factor(Fr), data = training)
summary(spline1m4)
pred.spline1m4 <- predict(spline1m4, testing)
mse_spline1m4 <- mean((pred.spline1m4 - log(testing$R_moment_4))^2)
spline2m4 <- lm(log(R_moment_4) ~ bs(log(St), df=4) + factor(Re) + factor(Fr), data = training)
summary(spline2m4)
pred.spline2m4 <- predict(spline2m4, testing)
mse_spline2m4 <- mean((pred.spline2m4 - log(testing$R_moment_4))^2)
spline3m4 <- lm(log(R_moment_4) ~ bs(log(St), df=5) + factor(Re) + factor(Fr), data = training)
summary(spline3m4)
pred.spline3m4 <- predict(spline3m4, testing)
mse_spline3m4 <- mean((pred.spline3m4 - log(testing$R_moment_4))^2)
spline4m4 <- lm(log(R_moment_4) ~ bs(log(St), df=6) + factor(Re) + factor(Fr), data = training)
summary(spline4m4)
pred.spline4m4 <- predict(spline4m4, testing)
mse_spline4m4 <- mean((pred.spline4m4 - log(testing$R_moment_4))^2)
spline5m4 <- lm(log(R_moment_4) ~ bs(log(St), df=7) + factor(Re) + factor(Fr), data = training)
summary(spline5m4)
pred.spline5m4 <- predict(spline5m4, testing)
mse_spline5m4 <- mean((pred.spline5m4 - log(testing$R_moment_4))^2)
mse_spline1m4
mse_spline2m4
mse_spline3m4
mse_spline4m4
mse_spline5m4
```


## Model for Predicting New Inputs

```{r}
p1 = ggplot(data_train, aes(x=St, y=log(R_moment_4), color = factor(Re))) +
  geom_point() +
  labs(title = "log(Moment 4) versus St, Color-Coded by Re")
p2 = ggplot(filter(data_train, Fr==0.052), aes(x=St, y=log(R_moment_4), color = factor(Re))) +
  geom_point() +
  labs(title = "log(Moment 4) versus St, Color-Coded by Re", subtitle = "Filtered by observations where Fr == 0.052")
p3 = ggplot(filter(data_train, Fr==0.3), aes(x=St, y=log(R_moment_4), color = factor(Re))) +
  geom_point() +
  labs(title = "log(Moment 4) versus St, Color-Coded by Re", subtitle = "Filtered by observations where Fr == 0.3")
p4 = ggplot(filter(data_train, Fr==Inf), aes(x=St, y=log(R_moment_4), color = factor(Re))) +
  geom_point() +
  labs(title = "log(Moment 4) versus St, Color-Coded by Re", subtitle = "Filtered by observations where Fr == Inf")
eda.plots = ggarrange(p1, p2, p3, p4, ncol=2, nrow=2)
eda.plots
```


```{r, results='asis'}
# perform transformation on Fr to get rid of infinity
g <- function(x) {
  return (1 - 2^(-x))
}

printModels <- function(models) {
  for (i in 1:length(models)) {
    print(paste0("R Moment ", i))
    print(knitr::kable(tidy(models[[i]]),
                       digits=3))
    stats = matrix(c(summary(models[[i]])$adj.r.squared, summary(models[[i]])$sigma),
                   ncol=2)
    colnames(stats) = c("Adj R^2", "Std Err")
    print(knitr::kable(stats, digits=3))
    cat('\n\n<!-- -->\n\n')
  }
}
```


```{r}
moments = list(data_train$R_moment_1, data_train$R_moment_2, data_train$R_moment_3, data_train$R_moment_4)
```

Try base linear model:

```{r, results='asis'}
lm.models = vector("list",4)
for (i in 1:4) {
  model = lm(log(moments[[i]]) ~ St + Re + g(Fr), data=data_train)
  lm.models[[i]] = model
}
printModels(lm.models)
```

St doesn't even seem to be a significant predictor in some of these models. Linear regression might not work super well here. 

Including all interaction terms:

```{r, results='asis'}
glm.full.models = vector("list",4)
for (i in 1:4) {
  model = lm(log(moments[[i]]) ~ (St + Re + g(Fr))^2, data=data_train)
  glm.full.models[[i]] = model
}
printModels(glm.full.models)
```

Re and g(Fr) appear to be significant, but St doesn't have any significant interactions.


A model with the interaction term for Re and Fr:

```{r, results='asis'}
glm.interaction.models = vector("list",4)
for (i in 1:4) {
  model = lm(log(moments[[i]]) ~ St + Re*g(Fr), data=data_train)
  glm.interaction.models[[i]] = model
}
printModels(glm.interaction.models)
```

None of these models are performing well at all, let's try other methods.

# Linear model using least squares & no interaction term 

```{r}
model1 <- lm(log(R_moment_1) ~ St + Re + g(Fr), data=training)
pred.lm1 <- predict(model1, testing)
mse_test1 <- mean((pred.lm1 - log(testing$R_moment_1))^2)
model2 <- lm(log(R_moment_2) ~ St + Re + g(Fr), data=training)
pred.lm2 <- predict(model2, testing)
mse_test2 <- mean((pred.lm2 - log(testing$R_moment_2))^2)
model3 <- lm(log(R_moment_3) ~ St + Re + g(Fr), data=training)
pred.lm3 <- predict(model3, testing)
mse_test3 <- mean((pred.lm3 - log(testing$R_moment_3))^2)
model4 <- lm(log(R_moment_4) ~ St + Re + g(Fr), data=training)
pred.lm4 <- predict(model4, testing)
mse_test4 <- mean((pred.lm4 - log(testing$R_moment_4))^2)
mse_test = matrix(c(mse_test1, mse_test2, mse_test3, mse_test4), ncol=4)
colnames(mse_test) = c("MSE 1", "MSE 2", "MSE 3", "MSE 4")
kable(mse_test, digits=3)
```

# Linear model using least squares & interaction term

```{r}
model1 <- lm(log(R_moment_1) ~ St + Re*g(Fr), data=training)
pred.lm1 <- predict(model1, testing)
mse_test1 <- mean((pred.lm1 - log(testing$R_moment_1))^2)
model2 <- lm(log(R_moment_2) ~ St + Re*g(Fr), data=training)
pred.lm2 <- predict(model2, testing)
mse_test2 <- mean((pred.lm2 - log(testing$R_moment_2))^2)
model3 <- lm(log(R_moment_3) ~ St + Re*g(Fr), data=training)
pred.lm3 <- predict(model3, testing)
mse_test3 <- mean((pred.lm3 - log(testing$R_moment_3))^2)
model4 <- lm(log(R_moment_4) ~ St + Re*g(Fr), data=training)
pred.lm4 <- predict(model4, testing)
mse_test4 <- mean((pred.lm4 - log(testing$R_moment_4))^2)
mse_test = matrix(c(mse_test1, mse_test2, mse_test3, mse_test4), ncol=4)
colnames(mse_test) = c("MSE 1", "MSE 2", "MSE 3", "MSE 4")
kable(mse_test, digits=3)
```


Really bad results, interaction terms aren't helping much. Let's try some other methods other than simple linear regression.


### Splines

```{r, message = FALSE, warning = FALSE}
spline1 <- lm(log(R_moment_1) ~ bs(log(St)) + bs(Re) + bs(g(Fr)), data = training)
pred.spline1 <- predict(spline1, testing)
mse_spline1 <- mean((pred.spline1 - log(testing$R_moment_1))^2)
r1=summary(spline1)$adj.r.squared

spline2 <- lm(log(R_moment_2) ~ bs(log(St)) + bs(Re) + bs(g(Fr)), data = training)
pred.spline2 <- predict(spline2, testing)
mse_spline2 <- mean((pred.spline2 - log(testing$R_moment_2))^2)
r2=summary(spline2)$adj.r.squared

spline3 <- lm(log(R_moment_3) ~ bs(log(St)) + bs(Re) + bs(g(Fr)), data = training)
pred.spline3 <- predict(spline3, testing)
mse_spline3 <- mean((pred.spline3 - log(testing$R_moment_3))^2)
r3=summary(spline3)$adj.r.squared

spline4 <- lm(log(R_moment_4) ~ bs(log(St)) + bs(Re) + bs(g(Fr)), data = training)
pred.spline4 <- predict(spline4, testing)
mse_spline4 <- mean((pred.spline4 - log(testing$R_moment_4))^2)
r4=summary(spline4)$adj.r.squared

p1 = ggplot(testing, aes(x = St, y = log(R_moment_1), color = factor(Re))) +
  geom_point() +
  geom_line(aes(x = St, y = pred.spline1), size = 1) +
  labs(title = "Spline Predictions - Moment 1")
p2 = ggplot(testing, aes(x = St, y = log(R_moment_2), color = factor(Re))) +
  geom_point() +
  geom_line(aes(x = St, y = pred.spline2), size = 1) +
  labs(title = "Spline Predictions - Moment 2")
p3 = ggplot(testing, aes(x = St, y = log(R_moment_3), color = factor(Re))) +
  geom_point() +
  geom_line(aes(x = St, y = pred.spline3), size = 1) +
  labs(title = "Spline Predictions - Moment 3")
p4 = ggplot(testing, aes(x = St, y = log(R_moment_4), color = factor(Re))) +
  geom_point() +
  geom_line(aes(x = St, y = pred.spline4), size = 1) +
  labs(title = "Spline Predictions - Moment 4")
spline.graphs = ggarrange(p1, p2, p3, p4, ncol=2, nrow=2)
spline.graphs

mse_spline = rbind(c(mse_spline1, mse_spline2, mse_spline3, mse_spline4), c(r1, r2, r3, r4))
colnames(mse_spline) = c("Moment 1", "Moment 2", "Moment 3", "Moment 4")
rownames(mse_spline) = c("MSE", "Ajd R^2")
kable(mse_spline, digits=3)
```


Splines definitely perform better than linear regression, but still room for improvement.

### Splines with interaction between Fr and Re

```{r, message = FALSE, warning = FALSE}
spline1 <- lm(log(R_moment_1) ~ bs(log(St)) + bs(Re)*bs(g(Fr)), data = training)
pred.spline1 <- predict(spline1, testing)
mse_spline1 <- mean((pred.spline1 - log(testing$R_moment_1))^2)
r1=summary(spline1)$adj.r.squared

spline2 <- lm(log(R_moment_2) ~ bs(log(St)) + bs(Re)*bs(g(Fr)), data = training)
pred.spline2 <- predict(spline2, testing)
mse_spline2 <- mean((pred.spline2 - log(testing$R_moment_2))^2)
r2=summary(spline2)$adj.r.squared

spline3 <- lm(log(R_moment_3) ~ bs(log(St)) + bs(Re)*bs(g(Fr)), data = training)
pred.spline3 <- predict(spline3, testing)
mse_spline3 <- mean((pred.spline3 - log(testing$R_moment_3))^2)
r3=summary(spline3)$adj.r.squared

spline4 <- lm(log(R_moment_4) ~ bs(log(St)) + bs(Re)*bs(g(Fr)), data = training)
pred.spline4 <- predict(spline4, testing)
mse_spline4 <- mean((pred.spline4 - log(testing$R_moment_4))^2)
r4=summary(spline4)$adj.r.squared

p1 = ggplot(testing, aes(x = St, y = log(R_moment_1), color = factor(Re))) +
  geom_point() +
  geom_line(aes(x = St, y = pred.spline1), size = 1) +
  labs(title = "Spline Predictions - Moment 1", subtitle = "Fr + Re interaction")
p2 = ggplot(testing, aes(x = St, y = log(R_moment_2), color = factor(Re))) +
  geom_point() +
  geom_line(aes(x = St, y = pred.spline2), size = 1) +
  labs(title = "Spline Predictions - Moment 2", subtitle = "Fr + Re interaction")
p3 = ggplot(testing, aes(x = St, y = log(R_moment_3), color = factor(Re))) +
  geom_point() +
  geom_line(aes(x = St, y = pred.spline3), size = 1) +
  labs(title = "Spline Predictions - Moment 3", subtitle = "Fr + Re interaction")
p4 = ggplot(testing, aes(x = St, y = log(R_moment_4), color = factor(Re))) +
  geom_point() +
  geom_line(aes(x = St, y = pred.spline4), size = 1) +
  labs(title = "Spline Predictions - Moment 4", subtitle = "Fr + Re interaction")
interaction.spline.graphs = ggarrange(p1, p2, p3, p4, ncol=2, nrow=2)
interaction.spline.graphs

mse_spline = rbind(c(mse_spline1, mse_spline2, mse_spline3, mse_spline4), c(r1, r2, r3, r4))
colnames(mse_spline) = c("Moment 1", "Moment 2", "Moment 3", "Moment 4")
rownames(mse_spline) = c("MSE", "Ajd R^2")
kable(mse_spline, digits=3)
```


Adding the interaction term to the splines makes the performance of this model superior to anything we've used previously (when considering numerical datapoints) based on Adj $R^2$ and MSE in test validation.



## Methodology (to include in writeup)


